# ğŸ™ï¸ğŸ¤Ÿ Traductor LSC â†’ Voz en Tiempo Real

Sistema completo que **detecta, reconoce y vocaliza**  
diez gestos bÃ¡sicos de la **Lengua de SeÃ±as Colombiana (LSC)**  
empleando Ãºnicamente una cÃ¡mara web y la **CPU**.

<p align="center">
  <img src="Figures/demo_gui.gif" width="600">
</p>

---

## ğŸ“Š Resultados

<table>
  <thead>
    <tr>
      <th style="text-align:left;">Conjunto</th>
      <th style="text-align:center;">Accuracy</th>
      <th style="text-align:center;">Macro-F<sub>1</sub></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Test</strong></td>
      <td style="text-align:center;">0.981</td>
      <td style="text-align:center;">0.981</td>
    </tr>
  </tbody>
</table>

---

## ğŸ—‚ï¸ Estructura del repositorio

```plaintext
.
â”œâ”€ capture_samples.py      # Paso 1 â€“ GrabaciÃ³n automÃ¡tica
â”œâ”€ normalize_samples.py    # Paso 2 â€“ InterpolaciÃ³n a 15 frames
â”œâ”€ create_keypoints.py     # Paso 3 â€“ ExtracciÃ³n de 1 662 key-points
â”œâ”€ prepare_dataset.py      # Paso 4 â€“ Split estratificado 70/15/15
â”œâ”€ model.py                # Paso 5 â€“ Red TCN + Attention
â”œâ”€ training_model.py       # Paso 6 â€“ Entrenamiento
â”œâ”€ confusion_matrix.py     # Paso 7 â€“ MÃ©tricas y grÃ¡ficas
â”œâ”€ plot_pr_curves.py       # ExtensiÃ³n: curvas PR
â”œâ”€ latent_tsne_umap.py     # ExtensiÃ³n: t-SNE / UMAP
â”œâ”€ main.py                 # Paso 8 â€“ GUI PyQt5 en tiempo real
â”œâ”€ text_to_speech.py       # Paso 9 â€“ SÃ­ntesis de voz
â”œâ”€ data/                   # Key-points y splits serializados
â”œâ”€ frame_actions/          # Frames JPG por gesto
â””â”€ models/                 # Modelo *.keras* y words.json


---

## ğŸ—‚ï¸ Estructura del repositorio

```

.
â”œâ”€ capture\_samples.py      # Paso 1 â€“ GrabaciÃ³n automÃ¡tica

â”œâ”€ normalize\_samples.py    # Paso 2 â€“ InterpolaciÃ³n a 15 frames

â”œâ”€ create\_keypoints.py     # Paso 3 â€“ ExtracciÃ³n de 1 662 key-points

â”œâ”€ prepare\_dataset.py      # Paso 4 â€“ Split estratificado 70/15/15

â”œâ”€ model.py                # Paso 5 â€“ Red TCN + Attention

â”œâ”€ training\_model.py       # Paso 6 â€“ Entrenamiento

â”œâ”€ confusion\_matrix.py     # Paso 7 â€“ MÃ©tricas y grÃ¡ficas

â”œâ”€ plot\_pr\_curves.py       # ExtensiÃ³n: curvas PR

â”œâ”€ latent\_tsne\_umap.py     # ExtensiÃ³n: t-SNE / UMAP

â”œâ”€ main.py                 # Paso 8 â€“ GUI PyQt5 en tiempo real

â”œâ”€ text\_to\_speech.py       # Paso 9 â€“ SÃ­ntesis de voz

â”œâ”€ data/                   # Key-points y splits serializados

â”œâ”€ frame\_actions/          # Frames JPG por gesto

â””â”€ models/                 # Modelo *.keras* y words.json

````

---

## âš¡ InstalaciÃ³n rÃ¡pida

```bash
# 1â€‚Crear y activar entorno virtual
python -m venv lsc_env
# Windows âœ lsc_env\Scripts\activate
source lsc_env/bin/activate

# 2â€‚Instalar dependencias (CPU-only)
pip install -r requirements.txt
#   â€“ TensorFlow 2.16.1
#   â€“ mediapipe
#   â€“ opencv-python
#   â€“ PyQt5
#   â€“ gTTS, pygame
#   â€“ seaborn, umap-learn â€¦

# 3â€‚Descargar modelo pre-entrenado
curl -L -o models/actions_15.keras https://â€¦/actions_15.keras
````

> El sistema corre a \~25 fps en un portÃ¡til i5; **no se requiere GPU**.

---

## â–¶ï¸ Demo en vivo

```bash
python main.py
```

1. Se abre la webcam.
2. Realice un gesto completo; la aplicaciÃ³n lo detecta automÃ¡ticamente.
3. Al acabar, se muestra el texto y se reproduce la voz en castellano.

---

## ğŸ”¬ Entrenamiento desde cero

```bash
# 1â€‚Capturar datos (â‰ˆ200 muestras por gesto)
python capture_samples.py --word hola

# 2â€‚Normalizar e indexar key-points
python normalize_samples.py
python create_keypoints.py

# 3â€‚Generar splits y entrenar
python prepare_dataset.py
python training_model.py            # produce models/actions_15.keras
```

---

## ğŸ“ˆ EvaluaciÃ³n y grÃ¡ficos adicionales

```bash
python confusion_matrix.py          # matriz + reporte
python plot_pr_curves.py            # curvas Precisionâ€“Recall
python latent_tsne_umap.py          # mapa t-SNE / UMAP
```

---

<h2>âœ¨ Componentes clave del sistema</h2>

<table>
  <thead>
    <tr>
      <th>MÃ³dulo</th>
      <th>TecnologÃ­a</th>
      <th>Rol principal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ğŸ§â€â™‚ï¸ DetecciÃ³n / Tracking</td>
      <td>MediaPipe</td>
      <td>1 662 landmarks (pose + face + hands)</td>
    </tr>
    <tr>
      <td>â±ï¸ NormalizaciÃ³n temporal</td>
      <td>OpenCV</td>
      <td>InterpolaciÃ³n / muestreo a 15 frames</td>
    </tr>
    <tr>
      <td>ğŸ§  ClasificaciÃ³n secuencial</td>
      <td><strong>TCN + Attention</strong></td>
      <td>RF 31 frames, 3.5 M parÃ¡metros, 98% accuracy</td>
    </tr>
    <tr>
      <td>ğŸ–¥ï¸ Interfaz grÃ¡fica</td>
      <td>PyQt5</td>
      <td>Webcam, overlay de keypoints, texto dinÃ¡mico</td>
    </tr>
    <tr>
      <td>ğŸ”Š SÃ­ntesis de voz</td>
      <td>gTTS + pygame</td>
      <td>LocuciÃ³n en espaÃ±ol con baja latencia</td>
    </tr>
  </tbody>
</table>


---

## ğŸ“ DescripciÃ³n breve

El proyecto integra **detecciÃ³n corporal, seguimiento temporal,
normalizaciÃ³n, red convolucional dilatada con atenciÃ³n escalar y
sÃ­ntesis de voz** para ofrecer un traductor LSC-a-Audio portÃ¡til,
de cÃ³digo abierto y totalmente funcional en CPU.

---

## ğŸ“š CrÃ©ditos

**Autores:**  Juan Sebastian Rodriguez Salazar y Johan Santiago Caro Valencia / Grupo â€“ VisiÃ³n Computacional, 2025-I

Agradecimientos al repositorio base [@ronvidev](https://github.com/ronvidev/modelo_lstm_lsp)
por la lÃ³gica original de captura.

```
```
